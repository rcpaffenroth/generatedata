# Task 3 of 3 — Add `load_data_as_sequence()` to `load_data.py`

## Series context

This is the third and final task adding time-series support to `generatedata`.

- **Task 1** added `seq_len` and `step_size` fields to `save_data()` / `info.json`.
- **Task 2** added `generate_mnist_sequence()` and `generate_mnist1d_sequence()` which
  populate those fields.
- **This task** adds the load function that consumes them.

## Background

`generatedata/load_data.py` currently exposes three public load functions:

| Function | Returns |
|---|---|
| `load_data(name, ...)` | `dict` with `"start"`, `"target"`, `"info"` keys (DataFrames) |
| `load_data_as_xy(name, ...)` | `tuple[pd.DataFrame, pd.DataFrame]` — `(X, Y)` |
| `load_data_as_xy_onehot(name, ...)` | same, gated on `onehot_y == 1` in metadata |

All three functions are unchanged by this task.

### Current flat layout of a sequence dataset (as stored by Task 2)

For `MNIST_seq1` the `target` parquet has shape `(num_points, 794)`:

```
columns : x0   … x783    x784  … x793
content : pixel[0]…pixel[783]  label[0]…label[9]
metadata: x_y_index=784, seq_len=784, step_size=1
```

## Function to add

Add the following function to `generatedata/load_data.py`, after
`load_data_as_xy_onehot`:

```python
def load_data_as_sequence(
    name: str,
    local: bool = False,
    data_dir: Path | str | None = None,
    label_every_step: bool = True,
) -> tuple[np.ndarray, np.ndarray]:
```

### Behaviour

1. **Load the flat data** using the existing `load_data()` helper (do not duplicate
   file-loading logic).

2. **Validate metadata**: the dataset's `info` dict must contain `seq_len` and
   `step_size`.  If either is missing, raise:
   ```
   ValueError: Dataset 'MNIST' has no sequence metadata (seq_len / step_size).
   Use load_data_as_xy() instead, or load a sequence variant such as 'MNIST_seq1'.
   ```

3. **Split pixels from labels** using `x_y_index`:
   ```python
   pixels = target_df.iloc[:, :x_y_index].to_numpy()   # (num_points, total_pixels)
   labels = target_df.iloc[:, x_y_index:].to_numpy()   # (num_points, label_dim)
   ```

4. **Reshape pixel array** into `(num_points, seq_len, step_size)`:
   ```python
   X_seq = pixels.reshape(num_points, seq_len, step_size)
   ```

5. **Handle labels** based on the `label_every_step` argument:

   - **`label_every_step=True` (default)**: broadcast the label vector across all
     timesteps and concatenate with the pixel sequence:
     ```python
     # labels shape: (num_points, label_dim)
     # → (num_points, seq_len, label_dim)
     labels_broadcast = np.broadcast_to(
         labels[:, np.newaxis, :], (num_points, seq_len, label_dim)
     ).copy()
     X_seq = np.concatenate([X_seq, labels_broadcast], axis=2)
     # final shape: (num_points, seq_len, step_size + label_dim)
     ```

   - **`label_every_step=False`**: return labels separately, unchanged:
     ```python
     # X_seq shape: (num_points, seq_len, step_size)
     # labels shape: (num_points, label_dim)   ← second return value
     ```

6. **Return value** is always `(X_seq, labels)` — a two-tuple of numpy arrays.
   - When `label_every_step=True`:
     - `X_seq.shape == (num_points, seq_len, step_size + label_dim)`
     - `labels.shape == (num_points, label_dim)` (unchanged; label also embedded in X_seq)
   - When `label_every_step=False`:
     - `X_seq.shape == (num_points, seq_len, step_size)`
     - `labels.shape == (num_points, label_dim)`

   Returning labels in both cases gives callers a consistent two-tuple API regardless of
   the `label_every_step` flag.

### Applying the same reshape to `start` data

The function should do the identical reshape on `start_df` as it does on `target_df` and
return **start** as the first element and **target** as the second, matching the
convention of `load_data_as_xy`:

```python
# Return (X_start_seq, X_target_seq) not (pixels_seq, labels)
# Labels from the target are used as the shared label for both.
```

Wait — re-read the convention.  `load_data_as_xy` returns `(X, Y)` where `X` is the
feature columns and `Y` is the label columns, both from `target`.  For sequence data the
return convention should be:

```python
return X_seq, labels   # both derived from target_df, same as load_data_as_xy
```

If callers need the `start` sequence they can access it via `load_data()` directly.

## Import to add

Add `import numpy as np` to the top of `load_data.py` if it is not already present.

## File to modify

`generatedata/load_data.py` only — add the new function and the `numpy` import.

## What NOT to change

- `save_data.py`, `data_generators.py`, or any other file.
- The existing `load_data`, `load_data_as_xy`, `load_data_as_xy_onehot` functions.

## Verification

Given a dataset `MNIST_seq1` generated by Task 2 with `num_points=100`:

```python
X_seq, labels = load_data_as_sequence("MNIST_seq1", local=True, data_dir=data_dir)
assert X_seq.shape == (100, 784, 11)    # 1 pixel + 10 label dims per step
assert labels.shape == (100, 10)

X_seq2, labels2 = load_data_as_sequence("MNIST_seq1", local=True, data_dir=data_dir,
                                         label_every_step=False)
assert X_seq2.shape == (100, 784, 1)    # pixels only
assert labels2.shape == (100, 10)
```

Calling it on a non-sequence dataset must raise `ValueError` with a helpful message:

```python
load_data_as_sequence("MNIST", local=True, data_dir=data_dir)
# → ValueError: Dataset 'MNIST' has no sequence metadata ...
```
